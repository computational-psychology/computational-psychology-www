************************************************************************
Can Convolutional Neural Networks benefit from fixational eye movements?
************************************************************************


:save_as: SP_CNNs_and_FEMs.html
:url: SP_CNNs_and_FEMs.html


Project Description
######################################################################

Human vision is mediated by neural responses to luminance discontinuities, i.e edges, in space and time (e.g. object boundaries or flashing lights, e.g. `[1] <https://doi.org/10.1167/13.2.25>`__, `[2] <https://doi.org/10.1523/JNEUROSCI.0848-14.2014>`__). These spatiotemporal edge signals are the basis for higher-level visual processes such as object perception and object recognition. Convolutional Neural Networks (CNNs) are a powerful tool to model human object recognition. However, current CNNs mostly ignore the relevance of temporal transients for visual processing. 
Different than for CNNs, the input to the visual system is always moving. Even when the eyes are still, tiny eye jitters (i.e. fixational eye movements, FEMs) occur. These FEMs seem to be important for encoding edges in space and time (e.g. `[3] <https://dx.doi.org/10.1016%2Fj.tins.2015.01.005>`__), because they introduce visual transients over time that naturally emphasize edge signals in the visual input. Here, we want to explore whether CNNs could benefit from a more dynamic strategy to recognize objects in natural images. The aim of the project will be to set up a dynamic-processing pipeline inspired by FEMs and spatiotemporal characteristics of the human eye, and to compare the object recognition performance of this dynamic CNN with a classical CNN.

Students will have to

- Scan the literature for existing implementations of CNNs that use videos as inputs
- Train at least two CNNs on object recognition with a static and dynamic-processing pipeline

Students will learn

- about fundamental mechanisms of visual processing
- how to implement a dynamic-processing pipeline inspired by spatiotemporal characteristics of the human eye

Requirements

- Good programming skills in Python
- Good English proficiency
- Experience with deep learning and deep learning software (TensorFlow or Keras)

If you are interested, please contact Lynn Schmittwilken (L.Schmittwilken@tu-berlin.de)
